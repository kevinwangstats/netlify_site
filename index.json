[{"authors":null,"categories":null,"content":"As a kid, I was always fascinated with detective stories, and how one can make inferences on human behaviours based on simple observations and scientific principles. Today, I am a bioinformatics scientist at Illumina. In my day-to-day work, I use my statistical and data science skills to unravel complex variations to improve our understanding of biology and diseases. I love examining data, proposing hypotheses and making my data tell interesting and unexpected stories about the world we live in.\nI am active on Twitter, always posting my analytical adventures and technical tips. I also release some of my analytical work through GitHub and tidytuesday webpage. My broad interest is in developing and applying statistical tools to understand complex data. My general specialisation is in applied statistics, data science and statistical package development in R and tidyverse.\nI hold a PhD in statistics from the University of Sydney and worked as a research associate in 2019 and a statistician at CSL Behring in 2020. My research work focused on developing bioinformatics methods to enable prediction of patient clinical outcomes using omics data. In addition, I provided consultation and analytical insights to clinicians/biologists through numerous collaborative publications. I was a Postgraduate Teaching Fellow at the University of Sydney (2016-2019) and a contributor to the outreach program at the University with a strong record in workshops \u0026amp; seminars.\n  Download my CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.kevinwangstats.com/author/kevin-y.x.-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kevin-y.x.-wang/","section":"authors","summary":"As a kid, I was always fascinated with detective stories, and how one can make inferences on human behaviours based on simple observations and scientific principles. Today, I am a bioinformatics scientist at Illumina.","tags":null,"title":"Kevin Y.X. Wang","type":"authors"},{"authors":[],"categories":["DataVisualisation"],"content":"        Introduction This is a simple exercise to extract data from a Wikipedia page (List of busiest passenger air routes) and performing a basic data visualisation.\nWhy am I doing this? I somehow really got into aviation in the past 2 years. Due to the COVID travel restriction, I have spent days on YouTube watching planes taking off and landing. In a moment of self-indulgence, I would also like to add that Airbus 380 is such a beautiful engineering marvel and it is sad that COVID fastened the end of its production.\nAirbus 380 at Hong Kong airport, 2018\n Extracting data We can, in theory, copy and paste the data to an Excel sheet and then import the data. However, we will try to do something a bit fancier and use the rvest package to extract this data.\nThe only downside with extracting the data in this way is that if the webpage is updated, then our code might not work. Hence, I will also save a copy of this data in my GitHub.\nsuppressPackageStartupMessages({ library(xml2) library(rvest) library(tidyverse) }) webpage = xml2::read_html(\u0026quot;https://en.wikipedia.org/wiki/List_of_busiest_passenger_air_routes\u0026quot;) raw_tbl = webpage %\u0026gt;% html_element(\u0026quot;table\u0026quot;) %\u0026gt;% html_table() raw_tbl ## # A tibble: 50 x 7 ## Rank `Airport 1` `Airport 2` `Distance (km)` `2018[1]` `2017[2]` Type ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 Jeju Seoul-Gimpo 449 14,107,4… 13,460,3… Domest… ## 2 2 Sapporo Tokyo-Haneda 835 9,698,639 8,726,502 Domest… ## 3 3 Sydney Melbourne 705 9,245,392 9,090,941 Domest… ## 4 4 Fukuoka Tokyo-Haneda 889 8,762,547 7,864,000 Domest… ## 5 5 Mumbai Delhi 1150 7,392,155 7,129,943 Domest… ## 6 6 Hanoi Ho Chi Minh C… 1171 6,867,114 6,769,823 Domest… ## 7 7 Beijing Shanghai-Hong… 1081 6,518,997 6,833,684 Domest… ## 8 8 Hong Kong Taipei-Taoyuan 802 6,476,268 6,719,030 Intern… ## 9 9 Tokyo-Haneda Naha 1573 5,829,712 5,269,481 Domest… ## 10 10 Jakarta Surabaya 700 5,649,046 5,271,304 Domest… ## # … with 40 more rows ## We are only interested in the 2018 passenger numbers subset_tbl = raw_tbl %\u0026gt;% dplyr::transmute( rank = Rank, airport1 = `Airport 1`, airport2 = `Airport 2`, distance = `Distance (km)`, passengers = `2018[1]` %\u0026gt;% str_remove_all(\u0026quot;,\u0026quot;) %\u0026gt;% as.integer(), type = Type) subset_tbl ## # A tibble: 50 x 6 ## rank airport1 airport2 distance passengers type ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 Jeju Seoul-Gimpo 449 14107414 Domestic ## 2 2 Sapporo Tokyo-Haneda 835 9698639 Domestic ## 3 3 Sydney Melbourne 705 9245392 Domestic ## 4 4 Fukuoka Tokyo-Haneda 889 8762547 Domestic ## 5 5 Mumbai Delhi 1150 7392155 Domestic ## 6 6 Hanoi Ho Chi Minh City 1171 6867114 Domestic ## 7 7 Beijing Shanghai-Hongqiao 1081 6518997 Domestic ## 8 8 Hong Kong Taipei-Taoyuan 802 6476268 International ## 9 9 Tokyo-Haneda Naha 1573 5829712 Domestic ## 10 10 Jakarta Surabaya 700 5649046 Domestic ## # … with 40 more rows write_csv(x = subset_tbl, file = \u0026quot;raw_airports_data.csv\u0026quot;) Notice that most of the “airports” are actually just the name of the city. We will use this to grab the longitude and latitude information. However, there are some exceptions like “Tokyo-Haneda”, where “Haneda” is one of the two international airports in the city of Tokyo. We will need to clean up these exceptions for consistency.\nclean_tbl = subset_tbl %\u0026gt;% dplyr::mutate( city1 = purrr::map_chr(.x = airport1, .f = ~ str_split(.x, \u0026quot;-\u0026quot;)[[1]][1]), city2 = purrr::map_chr(.x = airport2, .f = ~ str_split(.x, \u0026quot;-\u0026quot;)[[1]][1])) clean_tbl ## # A tibble: 50 x 8 ## rank airport1 airport2 distance passengers type city1 city2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 Jeju Seoul-Gimpo 449 14107414 Domestic Jeju Seoul ## 2 2 Sapporo Tokyo-Haneda 835 9698639 Domestic Sapporo Tokyo ## 3 3 Sydney Melbourne 705 9245392 Domestic Sydney Melbourne ## 4 4 Fukuoka Tokyo-Haneda 889 8762547 Domestic Fukuoka Tokyo ## 5 5 Mumbai Delhi 1150 7392155 Domestic Mumbai Delhi ## 6 6 Hanoi Ho Chi Minh… 1171 6867114 Domestic Hanoi Ho Chi M… ## 7 7 Beijing Shanghai-Ho… 1081 6518997 Domestic Beijing Shanghai ## 8 8 Hong Kong Taipei-Taoy… 802 6476268 Internat… Hong K… Taipei ## 9 9 Tokyo-Han… Naha 1573 5829712 Domestic Tokyo Naha ## 10 10 Jakarta Surabaya 700 5649046 Domestic Jakarta Surabaya ## # … with 40 more rows  Getting locations for the cities Google Maps API There are many ways of getting the location information for cities. In the past, I have found the most reliable way is to get it through ggmap which uses the Google Maps API, but this means you must set up a Google Cloud Platform billing account with them (which unfortunately requires a credit card). See this documentation. Once a project is set up with the Google Cloud Platform, you will then need to enable the Google Maps API by searching for it in the top search bar. The API key is required too, see the documentations for ggmap::register_google for more information.\nAside: is all these worth it? In my experience, absolutely! Because Google Maps is very smart and tends to understand certain complexities that you didn’t think of and handle those for you. For example, if you are interested in the city of Sydney, Google Maps will understand that to be the city of Sydney in Australia, not the city in Nova Scotia, Canada (I don’t know how they do this, but my guess is that they will return results that are more relevant, because, well, they are Google). Google Cloud is also offering free credits for most of their basic services, so one can take advantage of these without incurring substantial costs.\nTo ensure code reproducibility, I will use the code below to download the coordinates for all the cities, save it as a CSV and make it available on GitHub.\nlibrary(ggmap) # ggmap::geocode(\u0026quot;Sydney, Australia\u0026quot;, output = \u0026quot;latlon\u0026quot;, source = \u0026quot;google\u0026quot;) all_cities = c(clean_tbl$city1, clean_tbl$city2) %\u0026gt;% unique all_geocode = ggmap::geocode(location = all_cities, output = \u0026quot;latlon\u0026quot;) city_tbl = tibble( city = all_cities, lon = all_geocode$lon, lat = all_geocode$lat) readr::write_csv(x = city_tbl, file = \u0026quot;./city_tbl.csv\u0026quot;) Alternatively, if you don’t want to register for Google’s billings, you could use the tidygeocoder’s geocode function to get the latitude/longitude information via Open Street Map, which doesn’t require registration, but in my experience, it can be slower than Google Maps.\n tidygeocoder location extractions A small example:\ncity_tbl = tibble( city = c(clean_tbl$city1, clean_tbl$city2) %\u0026gt;% unique) %\u0026gt;% tidygeocoder::geocode(city, method = \u0026#39;osm\u0026#39;, lat = latitude , long = longitude)  Simple maps location extractions You could also use the data provided in https://simplemaps.com/data/world-cities to perform data joins to get the location information. The downloaded data looks quite tidy with additional ASCII encoding and I was quite impressed with the quality of the data.\n  Joining data and visualise Once we have the location information we can simply join the data as followed:\ncity_tbl = readr::read_csv(file = \u0026quot;https://gist.githubusercontent.com/kevinwang09/006d00ee7a43778171e7fc2fd409cdd6/raw/f12ec9deecdd99b093d0c819e21b125a7f7d4afd/city_tbl.csv\u0026quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## city = col_character(), ## lon = col_double(), ## lat = col_double() ## ) joined_data = clean_tbl %\u0026gt;% left_join(city_tbl, by = c(\u0026quot;city1\u0026quot; = \u0026quot;city\u0026quot;)) %\u0026gt;% left_join(city_tbl, by = c(\u0026quot;city2\u0026quot; = \u0026quot;city\u0026quot;), suffix = c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;)) I really like plotly’s globe visualisation, because you can click and drag the globe, which is really nice.\nUsing this visualisation, we can see most of the busiest routes in the world are concentrated around Asia. What surprised me a few years ago is that Australia, despite its small population, also had a couple of routes made it to this list, with Sydney - Melbourne being the third on the list. In my experience, on a good day, there could be two planes at the Sydney airport flying to Melbourne but only 10 minutes apart. Which I must admit was a rare luxury that I never realised.\nlibrary(plotly) ## ## Attaching package: \u0026#39;plotly\u0026#39; ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## last_plot ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## filter ## The following object is masked from \u0026#39;package:graphics\u0026#39;: ## ## layout geo \u0026lt;- list( scope = \u0026#39;world\u0026#39;, projection = list(type = \u0026#39;orthographic\u0026#39;), showland = TRUE, landcolor = toRGB(\u0026quot;gray95\u0026quot;), countrycolor = toRGB(\u0026quot;gray80\u0026quot;) ) fig \u0026lt;- plot_geo(color = I(\u0026quot;red\u0026quot;)) fig \u0026lt;- fig %\u0026gt;% add_markers( data = joined_data, x = ~lon1, y = ~lat1, text = ~city1, hoverinfo = \u0026quot;text\u0026quot;, alpha = 0.5) %\u0026gt;% add_markers( data = joined_data, x = ~lon2, y = ~lat2, text = ~city2, hoverinfo = \u0026quot;text\u0026quot;, alpha = 0.5) %\u0026gt;% add_segments( data = group_by(joined_data, rank), x = ~lon1, xend = ~lon2, y = ~lat1, yend = ~lat2, alpha = 0.3, size = I(1), hoverinfo = \u0026quot;none\u0026quot;) %\u0026gt;% layout( title = \u0026#39;Busiest air routes in the world\u0026#39;, geo = geo, showlegend = FALSE, height = 800) fig  {\"x\":{\"visdat\":{\"12dae18202af8\":[\"function () \",\"plotlyVisDat\"],\"12dae67e50c27\":[\"function () \",\"data\"],\"12dae6e8109ba\":[\"function () \",\"data\"],\"12dae59a5cb03\":[\"function () \",\"data\"]},\"cur_data\":\"12dae59a5cb03\",\"attrs\":{\"12dae67e50c27\":{\"color\":[\"red\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":{},\"y\":{},\"type\":\"scatter\",\"mode\":\"markers\",\"text\":{},\"hoverinfo\":\"text\",\"alpha\":0.5,\"inherit\":true},\"12dae6e8109ba\":{\"color\":[\"red\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":{},\"y\":{},\"type\":\"scatter\",\"mode\":\"markers\",\"text\":{},\"hoverinfo\":\"text\",\"alpha\":0.5,\"inherit\":true},\"12dae59a5cb03\":{\"color\":[\"red\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":{},\"y\":{},\"xend\":{},\"yend\":{},\"type\":\"scatter\",\"mode\":\"lines\",\"alpha\":0.3,\"size\":[1],\"hoverinfo\":\"none\",\"inherit\":true}},\"layout\":{\"height\":800,\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"mapType\":\"geo\",\"title\":\"Busiest air routes in the world\",\"geo\":{\"domain\":{\"x\":[0,1],\"y\":[0,1]},\"scope\":\"world\",\"projection\":{\"type\":\"orthographic\"},\"showland\":true,\"landcolor\":\"rgba(242,242,242,1)\",\"countrycolor\":\"rgba(204,204,204,1)\"},\"showlegend\":false,\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"type\":\"scattergeo\",\"mode\":\"markers\",\"text\":[\"Jeju\",\"Sapporo\",\"Sydney\",\"Fukuoka\",\"Mumbai\",\"Hanoi\",\"Beijing\",\"Hong Kong\",\"Tokyo\",\"Jakarta\",\"Jakarta\",\"Jeddah\",\"Tokyo\",\"Chengdu\",\"Guangzhou\",\"Cancun\",\"Beijing\",\"Brisbane\",\"Jakarta\",\"Guangzhou\",\"Shanghai\",\"Bangalore\",\"Jakarta\",\"Jakarta\",\"Cape Town\",\"Kuala Lumpur\",\"São Paulo–Congonhas\",\"Hong Kong\",\"New York\",\"Bogotá\",\"Bangalore\",\"Los Angeles\",\"Bangkok\",\"Brisbane\",\"Cebu\",\"Hong Kong\",\"Mexico City\",\"Kolkata\",\"Da Nang\",\"Chiang Mai\",\"Seoul\",\"Cusco\",\"New York\",\"Jakarta\",\"Mexico City\",\"Jeju\",\"İzmir\",\"Hong Kong\",\"Guangzhou\",\"Hong Kong\"],\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"marker\":{\"color\":\"rgba(255,0,0,0.5)\",\"line\":{\"color\":\"rgba(255,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(255,0,0,0.5)\"},\"line\":{\"color\":\"rgba(255,0,0,0.5)\"},\"geo\":\"geo\",\"lat\":[33.4996213,43.0617713,-33.8688197,33.5901838,19.0759837,21.0277644,39.9041999,22.3193039,35.6761919,-6.2087634,-6.2087634,21.485811,35.6761919,30.572815,23.12911,21.161908,39.9041999,-27.4704528,-6.2087634,23.12911,31.230416,12.9715987,-6.2087634,-6.2087634,-33.9248685,3.139003,-23.6273246,22.3193039,40.7127753,4.7109886,12.9715987,34.0522342,13.7563309,-27.4704528,10.3156992,22.3193039,19.4326077,22.572646,16.0544068,18.7883439,37.566535,-13.53195,40.7127753,-6.2087634,19.4326077,33.4996213,38.423734,22.3193039,23.12911,22.3193039],\"lon\":[126.5311884,141.3544507,151.2092955,130.4016888,72.8776559,105.8341598,116.4073963,114.1693611,139.6503106,106.845599,106.845599,39.1925048,139.6503106,104.066801,113.264385,-86.8515279,116.4073963,153.0260341,106.845599,113.264385,121.473701,77.5945627,106.845599,106.845599,18.4240553,101.686855,-46.6565842,114.1693611,-74.0059728,-74.072092,77.5945627,-118.2436849,100.5017651,153.0260341,123.8854366,114.1693611,-99.133208,88.363895,108.2021667,98.9853008,126.9779692,-71.9674626,-74.0059728,106.845599,-99.133208,126.5311884,27.142826,114.1693611,113.264385,114.1693611],\"frame\":null},{\"type\":\"scattergeo\",\"mode\":\"markers\",\"text\":[\"Seoul\",\"Tokyo\",\"Melbourne\",\"Tokyo\",\"Delhi\",\"Ho Chi Minh City\",\"Shanghai\",\"Taipei\",\"Naha\",\"Surabaya\",\"Denpasar\",\"Riyadh\",\"Osaka\",\"Beijing\",\"Beijing\",\"Mexico City\",\"Shenzhen\",\"Sydney\",\"Singapore\",\"Shanghai\",\"Shenzhen\",\"Delhi\",\"Makassar\",\"Medan\",\"Johannesburg\",\"Singapore\",\"Rio de Janeiro\",\"Shanghai\",\"Los Angeles\",\"Medellin\",\"Mumbai\",\"San Francisco\",\"Phuket\",\"Melbourne\",\"Manila\",\"Bangkok\",\"Monterrey\",\"Delhi\",\"Ho Chi Minh City\",\"Bangkok\",\"Osaka\",\"Lima\",\"Chicago\",\"Kuala Lumpur\",\"Guadalajara\",\"Gimhae\",\"Istanbul\",\"Seoul\",\"Chengdu\",\"Manila\"],\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"marker\":{\"color\":\"rgba(255,0,0,0.5)\",\"line\":{\"color\":\"rgba(255,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(255,0,0,0.5)\"},\"line\":{\"color\":\"rgba(255,0,0,0.5)\"},\"geo\":\"geo\",\"lat\":[37.566535,35.6761919,-37.8136276,35.6761919,28.7040592,10.8230989,31.230416,25.0329694,26.2125758,-7.2574719,-8.6704582,24.7135517,34.6937249,39.9041999,39.9041999,19.4326077,22.543096,-33.8688197,1.352083,31.230416,22.543096,28.7040592,-5.1476651,3.5951956,-26.2041028,1.352083,-22.9068467,31.230416,34.0522342,6.2476376,19.0759837,37.7749295,7.9519331,-37.8136276,14.5995124,13.7563309,25.6866142,28.7040592,10.8230989,13.7563309,34.6937249,-12.0463731,41.8781136,3.139003,20.6596988,35.2285451,41.0082376,37.566535,30.572815,14.5995124],\"lon\":[126.9779692,139.6503106,144.9630576,139.6503106,77.1024902,106.6296638,121.473701,121.5654177,127.6790208,112.7520883,115.2126293,46.6752957,135.5022535,116.4073963,116.4073963,-99.133208,114.057865,151.2092955,103.819836,121.473701,114.057865,77.1024902,119.4327314,98.6722227,28.0473051,103.819836,-43.1728965,121.473701,-118.2436849,-75.5658153,72.8776559,-122.4194155,98.3380884,144.9630576,120.9842195,100.5017651,-100.3161126,77.1024902,106.6296638,100.5017651,135.5022535,-77.042754,-87.6297982,101.686855,-103.3496092,128.8893517,28.9783589,126.9779692,104.066801,120.9842195],\"frame\":null},{\"type\":\"scattergeo\",\"mode\":\"lines\",\"hoverinfo\":[\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\",null,\"none\",\"none\"],\"marker\":{\"color\":\"rgba(255,0,0,0.3)\",\"size\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"sizemode\":\"area\",\"line\":{\"color\":\"rgba(255,0,0,1)\"}},\"textfont\":{\"color\":\"rgba(255,0,0,0.3)\",\"size\":1},\"line\":{\"color\":\"rgba(255,0,0,0.3)\",\"width\":1},\"geo\":\"geo\",\"lat\":[33.4996213,37.566535,null,43.0617713,35.6761919,null,-33.8688197,-37.8136276,null,33.5901838,35.6761919,null,19.0759837,28.7040592,null,21.0277644,10.8230989,null,39.9041999,31.230416,null,22.3193039,25.0329694,null,35.6761919,26.2125758,null,-6.2087634,-7.2574719,null,-6.2087634,-8.6704582,null,21.485811,24.7135517,null,35.6761919,34.6937249,null,30.572815,39.9041999,null,23.12911,39.9041999,null,21.161908,19.4326077,null,39.9041999,22.543096,null,-27.4704528,-33.8688197,null,-6.2087634,1.352083,null,23.12911,31.230416,null,31.230416,22.543096,null,12.9715987,28.7040592,null,-6.2087634,-5.1476651,null,-6.2087634,3.5951956,null,-33.9248685,-26.2041028,null,3.139003,1.352083,null,-23.6273246,-22.9068467,null,22.3193039,31.230416,null,40.7127753,34.0522342,null,4.7109886,6.2476376,null,12.9715987,19.0759837,null,34.0522342,37.7749295,null,13.7563309,7.9519331,null,-27.4704528,-37.8136276,null,10.3156992,14.5995124,null,22.3193039,13.7563309,null,19.4326077,25.6866142,null,22.572646,28.7040592,null,16.0544068,10.8230989,null,18.7883439,13.7563309,null,37.566535,34.6937249,null,-13.53195,-12.0463731,null,40.7127753,41.8781136,null,-6.2087634,3.139003,null,19.4326077,20.6596988,null,33.4996213,35.2285451,null,38.423734,41.0082376,null,22.3193039,37.566535,null,23.12911,30.572815,null,22.3193039,14.5995124],\"lon\":[126.5311884,126.9779692,null,141.3544507,139.6503106,null,151.2092955,144.9630576,null,130.4016888,139.6503106,null,72.8776559,77.1024902,null,105.8341598,106.6296638,null,116.4073963,121.473701,null,114.1693611,121.5654177,null,139.6503106,127.6790208,null,106.845599,112.7520883,null,106.845599,115.2126293,null,39.1925048,46.6752957,null,139.6503106,135.5022535,null,104.066801,116.4073963,null,113.264385,116.4073963,null,-86.8515279,-99.133208,null,116.4073963,114.057865,null,153.0260341,151.2092955,null,106.845599,103.819836,null,113.264385,121.473701,null,121.473701,114.057865,null,77.5945627,77.1024902,null,106.845599,119.4327314,null,106.845599,98.6722227,null,18.4240553,28.0473051,null,101.686855,103.819836,null,-46.6565842,-43.1728965,null,114.1693611,121.473701,null,-74.0059728,-118.2436849,null,-74.072092,-75.5658153,null,77.5945627,72.8776559,null,-118.2436849,-122.4194155,null,100.5017651,98.3380884,null,153.0260341,144.9630576,null,123.8854366,120.9842195,null,114.1693611,100.5017651,null,-99.133208,-100.3161126,null,88.363895,77.1024902,null,108.2021667,106.6296638,null,98.9853008,100.5017651,null,126.9779692,135.5022535,null,-71.9674626,-77.042754,null,-74.0059728,-87.6297982,null,106.845599,101.686855,null,-99.133208,-103.3496092,null,126.5311884,128.8893517,null,27.142826,28.9783589,null,114.1693611,126.9779692,null,113.264385,104.066801,null,114.1693611,120.9842195],\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}  ","date":1622678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622704978,"objectID":"6e0ae3a419e150aec936d7a2c42bed0e","permalink":"https://www.kevinwangstats.com/post/busiest-air-routes/","publishdate":"2021-06-03T00:00:00Z","relpermalink":"/post/busiest-air-routes/","section":"post","summary":"Introduction This is a simple exercise to extract data from a Wikipedia page (List of busiest passenger air routes) and performing a basic data visualisation.","tags":[],"title":"Busiest air routes","type":"post"},{"authors":[],"categories":["statistics"],"content":"  Introduction How can we (statisticians) better explain the differences between a confidence interval (CI) and a prediction interval (PI)? Sure, one could look up the definition on Wikipedia and memorise the definitions, but the real difficulty is how to communicate this clearly to young students/collaborators/clients without using mathematical formalism. For a linear regression model and a given value of the independent variable, the CI and PI confusingly share the same point estimate. Worse still, if we write out the mathematical formulas, they are virtually identical except one term!\nThe difference between the interpretation of CI and PI is actually a great example of how very similar mathematical constructions can lead to very different interpretations. These terms are not always rigorously defined and used, sometimes even in reputable sources (I would also add tolerance intervals here as well, but perhaps for another day).\nThis blog post explains the main statistical differences between CI and PI in a linear regression model through visualisations. In short:\n CI shows the variability in parameter estimates. The primary intention is to understand the variability in the model.\n PI shows the variability in individual data points. The primary intention is to capture future data points.\n  The statements above are of course extreme simplifications of these statistical concepts. I will attempt to minimise the need for mathematical derivations and use intuitive language and simulations to illustrate the subtle differences between these two concepts.\n A quick simulation to set the scene Suppose we have an independent variable (\\(x\\)) and a dependent variable (\\(y\\)) and we are asked to produce a linear regression.\nFor simplicity, I will generate some data with \\(X \\sim N(2,0.2^2)\\), \\(\\epsilon \\sim N(0, 0.2^2)\\) and \\(y = 1 + 2*x + \\epsilon\\). As much as I would like to use real data to add real-world relevance, generating data with a known value means that we are allowed to discuss how good our estimates are compared to the “true” value. This idea of a “true” value is not always possible if we use real data.\n## Some packages that we will be using suppressPackageStartupMessages({ library(tidyverse) library(broom) library(ggpubr) }) We will also use the geom_smooth(method = \"lm\") function from the ggplot2 package to add the (simple) linear regression line. I also choose to use the option se = FALSE to suppress the visualisation of the (confidence) interval as we will do this manually later.\nset.seed(8) theme_set(theme_classic(18)) n = 30 x = rnorm(n, 2, 0.2) epsilon = rnorm(n, 0, 0.2) y = 1 + 2*x + epsilon df = tibble(x, y) model = lm(y ~ x, data = df) df %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + ggpubr::stat_regline_equation() ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;  Confidence interval (CI) On a single parameter Definition: if we are asked to construct a 95% CI for a parameter, then the probability* that this CI will contain the true population parameter value is 95%. We can replace the “95%” with any other percentage that we can think of, though it is rare for us to consider something below 90%.\n*The word “probability” is to be interpreted using a frequentist approach. See bonus section 1.\nFor example, if we were to look at the linear model above, the true slope value is 2, because that is how we generated the data through a formula. But the point estimate for this parameter is about 2.1 (formula in the top left corner of the plot), not exactly 2. You may ask, what is going on?!\nThis is because any data sampled from a population, being only a sample, cannot encapsulate entirely what is going on in the population. Our point estimate (in this case, 2.1) is estimated using our data, and thus, can’t be numerically identical to the true parameter (in this case, 2) that is associated with a population. This interplay between an estimate and the parameter of interest (or similarly, between a sampled data and the underlying population) is a fundamental concept in statistics.\nWe can think of a CI as being another estimate for the population parameter. Unlike a point estimate, which is just a point, a CI is an interval defined between an upper bound and a lower bound. The midpoint of a CI is the point estimate. Thus, we can think of a CI as a way to quantify the variability for that point estimate*!\n*Interestingly, in my experience, a point estimate, being just a single number, can often mislead some people into thinking that there is a lot of certainty behind this number when it absolutely does not! The associate CI could be quite large even if your point estimate looks very reasonable.\nThe confidence interval for the slope parameter can be computed as (1.8067, 2.4091) using the code below. Notice how the point estimate of 2.1 is halfway between the two bounds and that this CI also contains the true parameter value of 2. Similarly, one can construct confidence interval for the intercept term as (0.1906, 1.3905).\nconfint(model) ## 2.5 % 97.5 % ## (Intercept) 0.1905894 1.390481 ## x 1.8067446 2.409144  Visualisation We now have a basic understanding of what a CI is with respect to a single parameter, but who cares about a single parameter anyways? After all, a statistical model could have many parameters, how would knowing a single one enhance our knowledge of the entire model?\nHere is where a bit of creative visualisation can help us! Notice how the linear regression model, written out as a formula, is \\(y = \\alpha + \\beta x + \\epsilon\\) with \\(\\alpha\\) and \\(\\beta\\) being the population intercept and slope parameters, respectively. When we make estimations on these parameters, our linear model, as a straight line is \\(y = \\hat{\\alpha} + \\hat{\\beta} x + \\epsilon\\), where \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) estimate \\(\\alpha\\) and \\(\\beta\\) respectively. So if we have the CIs quantifying the variability of\\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), then we should also be able to produce a CI associated with the linear model itself! And by doing so, we can get a better idea of how variable the entire model is because we know how variable the individual estimates are!\nEven if you didn’t understand the above, the CI associated with the linear model is simply the gray area produced by geom_smooth(method = \"lm\", se = TRUE) when using ggplot2! You can think of this line as how the linear model line (blue) “jiggles” if you have (future) data sampled from the same underlying distribution.\ndf %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_point() + ggpubr::stat_regline_equation() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = TRUE) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;   Prediction interval (PI) On a single data point Definition: If we were to construct a 95% prediction interval for a given value of \\(x\\) for a given linear model, then the probability that it will contain the corresponding value of \\(y\\) is 95%.\nThis is fundamentally different to a confidence interval, because CI tries to quantify the variability of an estimate, but PI is aiming to capture the variability of a (future) data point, \\(y_{new}\\), given some \\(x_{new}\\) value. In other words, a CI tries to quantify the behaviour of an estimate (obtained through averaging many data points), but a PI tries to quantify the behaviour of a single data point. Thus, we should expect, through simple intuition, that individual behaviour is more variable than the averaged behaviour. And so, PI is usually wider than a CI.\nThe main source of confusion for some people is that, both CI and PI share the same point estimate, i.e. \\(\\hat{\\alpha} + \\hat{\\beta} \\bar{x}\\). It also doesn’t help when these concepts are sometimes mixed up and talked about as though they are interchangeable in some colloquial settings.\nOne can compute the point prediction value and the 95% PI in R using the code below. Though admittedly, we are simply producing predictions for the original data, not new data (thus the warning message in R).\npredict(model, interval = \u0026quot;prediction\u0026quot;) %\u0026gt;% head ## Warning in predict.lm(model, interval = \u0026quot;prediction\u0026quot;): predictions on current data refer to _future_ responses ## fit lwr upr ## 1 4.970763 4.605474 5.336052 ## 2 5.360727 4.991068 5.730386 ## 3 4.811024 4.445082 5.176966 ## 4 4.774198 4.407903 5.140492 ## 5 5.316730 4.947982 5.685477 ## 6 4.960942 4.595653 5.326230   Visualisation In my experience, PI’s are more intuitive to understand than CI’s. PI is simply a region to capture new data, but CI is about how the model “jiggles”.\nIt is slightly harder to generate a PI using ggplot2 with some customisation needed for the geom_smooth function. Thus, I will attempt to generate both CI and PI manually and use ggplot2 for visualisation. I particularly like the broom::augment function to generate these intervals here.\nci = broom::augment(model, interval = \u0026quot;confidence\u0026quot;) pi = broom::augment(model, interval = \u0026quot;prediction\u0026quot;) plotdf = tibble( x = ci$x, y = ci$y, line = ci$.fitted, ci_lwr = ci$.lower, ci_upr = ci$.upper, pi_lwr = pi$.lower, pi_upr = pi$.upper) plot1 = plotdf %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = line), colour = \u0026quot;blue\u0026quot;) + geom_ribbon(aes(x = x, ymin = ci_lwr, ymax = ci_upr), fill = \u0026quot;gray20\u0026quot;, alpha = 0.3) + labs(caption = \u0026quot;blue: single estimated linear regression line, gray: 95% CI\u0026quot;) plot1 plot2 = plot1 + geom_ribbon(aes(x = x, ymin = pi_lwr, ymax = pi_upr), colour = \u0026quot;red\u0026quot;, alpha = 0) + labs(caption = \u0026quot;blue: single estimated linear regression line, red: 95% PI, gray: 95% CI\u0026quot;) plot2 From the plot, I would like to reiterate this idea that a CI (gray region) tries to quantify averaged behaviour of a model (blue line), and notice how small the jiggling is. But a PI (red lines) tries to quantify the behaviour of individual data points, and thus PI is much wider, because individuals are less predictable than the average.\n Bonus 1: probability and confidence level In all the writings above, I have always assumed that we already know what a probability/confidence level is. However, this might not be fair and I would like to explain this concept better in this section.\nIn most introductory statistics courses, the first definition of probability is likely to be the frequentist definition, which is closely related to the concept of (re-)sampling. In our definition of CI above, we used 95% confidence level as the default. This means that, if we were to repeatedly sample new data of the same sample size from the population, and construct the CI in the same way, then the proportion of times that the CIs containing the true population parameter is 95%. This idea of “repeated sampling” applies to both PI and TI. Thus, even though in the definitions above, we have used the words “probability” and “confidence level” interchangeably, but in the most rigorous way, these interval concepts should only be interpreted through repeated sampling. We will do this in bonus section 2.\n Bonus 2: visualisation through repetitions We will now repeat the data generation process 100 times and compute the corresponding statistics. The code below is not quite readable unless you are already familiar with tidyverse and nested tibble. The essence of the code is to first construct data in exactly the same way as we have above. And then, for each of the 100 data generation, we will then extract the corresponding fitted line, CI and PI for visualisation.\nlibrary(gganimate) nsim = 100 ## Number of simulations n = 30 sim_tbl = tibble( x = rnorm(n*nsim, 2, sd = 0.2), epsilon = rnorm(n*nsim, 0, sd = 0.2), y = 1 + 2*x + epsilon, sim_num = rep(1:nsim, n)) ## This tibble puts all the simulated data in one sim_tbl ## # A tibble: 3,000 x 4 ## x epsilon y sim_num ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2.11 0.122 5.34 1 ## 2 2.02 0.107 5.15 2 ## 3 1.64 0.151 4.44 3 ## 4 1.78 -0.0513 4.51 4 ## 5 1.95 -0.224 4.67 5 ## 6 1.78 0.159 4.73 6 ## 7 1.90 0.0888 4.89 7 ## 8 2.33 -0.101 5.55 8 ## 9 2.08 0.0578 5.22 9 ## 10 1.68 0.267 4.63 10 ## # … with 2,990 more rows sim_lm_tbl = sim_tbl %\u0026gt;% group_by(sim_num) %\u0026gt;% tidyr::nest() %\u0026gt;% ## Group by simulation number and separately fit linear models dplyr::mutate( lm = purrr::map(.x = data, .f = ~ lm(y ~ x, data = .x)), lm_tidy = purrr::map(lm, broom::tidy)) sim_lm_tbl ## # A tibble: 100 x 4 ## # Groups: sim_num [100] ## sim_num data lm lm_tidy ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 2 2 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 3 3 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 4 4 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 5 5 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 6 6 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 7 7 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 8 8 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 9 9 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## 10 10 \u0026lt;tibble[,3] [30 × 3]\u0026gt; \u0026lt;lm\u0026gt; \u0026lt;tibble[,5] [2 × 5]\u0026gt; ## # … with 90 more rows Fitted lines For each the linear models that we have fitted, we will now extract the coefficients. We will perform additional manipulations for the purpose of plotting.\nNotice how even though we have many fitted lines, their deviations from the single fitted line are small and almost cover the same range as the confidence interval (gray region). This is exactly what a confidence interval is designed for: to capture how a single fitted line jiggles under sampling from the same population.\nsim_coef_tbl = sim_lm_tbl %\u0026gt;% dplyr::select(sim_num, lm_tidy) %\u0026gt;% unnest(lm_tidy) %\u0026gt;% dplyr::select(sim_num, term, estimate) %\u0026gt;% tidyr::pivot_wider(names_from = \u0026quot;term\u0026quot;, values_from = \u0026quot;estimate\u0026quot;) sim_coef_tbl ## # A tibble: 100 x 3 ## # Groups: sim_num [100] ## sim_num `(Intercept)` x ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.484 2.23 ## 2 2 0.941 2.05 ## 3 3 1.38 1.82 ## 4 4 1.85 1.59 ## 5 5 0.972 2.03 ## 6 6 0.606 2.17 ## 7 7 0.911 2.06 ## 8 8 1.24 1.88 ## 9 9 0.524 2.24 ## 10 10 0.826 2.11 ## # … with 90 more rows plot1 + geom_abline(slope = 2, intercept = 1) + geom_abline(data = sim_coef_tbl, aes(slope = x, intercept = `(Intercept)`), size = 0.3, alpha = 0.5, colour = \u0026quot;orange\u0026quot;) + labs(title = \u0026quot;Fitted lines\u0026quot;, subtitle = \u0026quot;\u0026quot;, caption = \u0026quot;black: true linear relationship blue: single estimated linear regression line orange: 100 simulated linear regression lines, gray: 95% CI\u0026quot;)  Confidence intervals What I don’t like about the plot above is that orange lines are plotted over other elements of the plot, i.e. overplotting. Another attempt at the same visualisation is to use animation. I will use only the first 20 simulations.\nsim_coef_tbl20 = sim_coef_tbl %\u0026gt;% dplyr::filter(sim_num \u0026lt;= 20) sim_tbl20 = sim_tbl %\u0026gt;% dplyr::filter(sim_num \u0026lt;= 20) sim_conf_int = augment(model, newdata = sim_tbl20, interval = \u0026quot;confidence\u0026quot;) %\u0026gt;% dplyr::select(x, y, .lower, .upper) animation1 = ggplot() + geom_point(data = sim_tbl20, aes(x = x, y = y)) + geom_ribbon(data = sim_conf_int, aes(x = x, y = y, ymin = .lower, ymax = .upper), alpha = 0.5) + geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], colour = \u0026quot;blue\u0026quot;) + geom_abline(data = sim_coef_tbl20, aes(slope = x, intercept = `(Intercept)`), colour = \u0026quot;orange\u0026quot;) + transition_states(sim_num) + shadow_mark(exclude_layer = 1) + labs(title = \u0026quot;Effect of CI under simulations\u0026quot;, subtitle = \u0026quot;Simulation {closest_state}\u0026quot;, caption = \u0026quot;blue: single estimated linear regression line, orange: 20 simulated linear regression lines, gray: 95% CI for the blue line\u0026quot;) # animation1 # gganimate::anim_save(filename = \u0026quot;CI_animation.gif\u0026quot;, animation = animation1) sim_pred_int = augment(x = model, newdata = sim_tbl20, interval = \u0026quot;predict\u0026quot;) %\u0026gt;% dplyr::select(x, y, .lower, .upper) animation2 = sim_tbl20 %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_point(size = 1) + geom_ribbon(data = sim_pred_int, aes(x = x, ymin = .lower, ymax = .upper), fill = \u0026quot;gray20\u0026quot;, alpha = 0.5) + geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2], colour = \u0026quot;blue\u0026quot;) + transition_states(sim_num) + shadow_mark(exclude_layer = 1) + labs(title = \u0026quot;Effect of PI under simulations\u0026quot;, subtitle = \u0026quot;Simulation {closest_state}\u0026quot;, caption = \u0026quot;blue: single estimated linear regression line, \\n gray: 95% PI for the blue line\u0026quot;) # animation2 # gganimate::anim_save(filename = \u0026quot;PI_animation.gif\u0026quot;, animation = animation2)   Bonus 3: side by side animation This post shows how to put gganimate plots together.\nlibrary(patchwork) library(magick) a_mgif \u0026lt;- image_read(animate(animation1)) b_mgif \u0026lt;- image_read(animate(animation2)) new_gif \u0026lt;- image_append(c(a_mgif[1], b_mgif[1])) for(i in 2:20){ combined \u0026lt;- image_append(c(a_mgif[i], b_mgif[i])) new_gif \u0026lt;- c(new_gif, combined) } gganimate::anim_save(filename = \u0026quot;combined_CI_PI_animation.gif\u0026quot;, animation = new_gif)  Bonus 4: Mathematical formulas Ok, I promise this is the last bonus section. I am including some formulas for completeness here.\nSuppose that our data design matrix is \\(X \\in \\mathbb{R}^{n \\times p}\\) and the response variable is \\(y \\in \\mathbb{R}^n\\). If we set the confidence level to \\((1-\\alpha)\\%\\), then we will use \\(t_{n-p}^{(\\alpha / 2)}\\) to denote the Student t-distribution’s critical value, with \\(n-p\\) degrees of freedom.\nFor a new data point at \\((x_{\\text{new}}, y_{\\text{new}})\\), the estimated response is \\(\\hat{y}_{new} = a + b x_{new}\\), where \\(a\\) and \\(b\\) are estimated intercept and slope term respectively. The estimated linear regression residual standard deviation is \\(\\hat{\\sigma}\\).\nThe confidence interval under this notation set up is:\n\\[\\hat{y}_{new} \\pm t_{n-p}^{(\\alpha / 2)} \\hat{\\sigma} \\sqrt{x_{new}^{T}\\left(X^{T} X\\right)^{-1} x_{new}}.\\] The prediction interval under this notation set up is:\n\\[\\hat{y}_{new} \\pm t_{n-p}^{(\\alpha / 2)} \\hat{\\sigma} \\sqrt{1+x_{new}^{T}\\left(X^{T} X\\right)^{-1} x_{new}}.\\] Notice that both formulas share the same point estimate of \\(\\hat{y}_{new}\\) and the only difference between these two formulas is the term 1 under the square root sign. But that single term makes up all the differences between these statistical concepts!\n ","date":1622332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622363017,"objectID":"8e8996fd5bc3cbfab8ed99ad424028db","permalink":"https://www.kevinwangstats.com/post/2021-05-30-confidence-and-prediction-intervals/","publishdate":"2021-05-30T00:00:00Z","relpermalink":"/post/2021-05-30-confidence-and-prediction-intervals/","section":"post","summary":"Introduction How can we (statisticians) better explain the differences between a confidence interval (CI) and a prediction interval (PI)? Sure, one could look up the definition on Wikipedia and memorise the definitions, but the real difficulty is how to communicate this clearly to young students/collaborators/clients without using mathematical formalism.","tags":[],"title":"Confidence and prediction intervals","type":"post"},{"authors":[],"categories":[],"content":"  Introduction tl;dr: Google’s Data Studio is a great (and free) option for making data visualisation dashboards. Being a web-based dashboard application, its ability to connect to other Google products, such as BigQuery and GCP resources offers huge advantage over competing products. You can design a dashboard with minimum programming experience and the process is similar to writing PowerPoint slides. However, DS is mostly for the purpose of visualisation, so R/Shiny still has an advantage in computing and modeling. Below is my hacky 2-page report, correct up to April 2020:\n I am in the process of learning how to use BigQuery (some progress are documented here in this post). When I was exploring BigQuery’s web interface, I noticed that BigQuery could be connected with Data Studio (DS), a Google product that makes dashboard/BI visualisations.\n Some person comments As I recently have delivered a lecture on Shiny, I thought I will check out how Data Studio works and how it compares with Shiny. Sure, I have used Tableau before, but I have never used this kind of data visualisation software with databases before.\nOverall, I think DS is a great web-based tool for quick explorations/visualisation of the data, its ability to connect to BigQuery and other databases is very attractive. I almost can’t believe how smooth the connection was after I have tried to connect the same data into R. While many features that DS offers are not dissimilar to Microsoft Power BI and Tableau, DS is free to use! Which is quite important for, well, people like myself who just lost the privilege to claim student discounts.\nThe drawback of DS is also clear: it is not Shiny, so it won’t handle complex modelling/computations in the background (that being said, I am sure there is a way that you can connect it to Google Cloud products for backend computations). DS is also web-based, so connecting it with data in your local laptop isn’t quite straight forward. It is also not quite as reproducible since it is not constructed using scripts. But these shouldn’t overshadow how easy it was to use DS and share it.\nIf a single visualisation is all you are after, not complex modelling, then DS is definitely a faster option than Shiny with a gentler learning curve.\n Starting started with a DS report A DS report is made up of multiple pages, much like PowerPoint slides. This tutorial is quite helpful to learn the basics.\n The most powerful part of DS, in my opinion, is how you can add multiple data sources, e.g. SQL databases or Google Sheets. But being part of the Google ecosystem, I think the real power lies in how DS interacts with other Google products like Google Analytics and other third-party data sources.   In the DS report above, I made connections to the COVID-19 data from Johns Hopskins University and the World Bank global population data. I couldn’t find a Google public data documentation page for the latter, but you can make a SQL query using:  SELECT * FROM `bigquery-public-data.world_bank_global_population.population_by_country` LIMIT 10  Having added the these two data into the DS report, you can visualise these data using charts, such as a table on the page 1 of the report:   In DS, every chart is associated with one data source that you have added. Of course, you can put together multiple data in some meaningful way and make a visualisation. This is exactly what I have done to calculate the “confirmed cases per 1 million” statistic.\n A bit more data manipulations In order to make the heatmap at the beginning of this post, a bit more data manipulations are needed. In particular, joining the two data together using “country” as a common key. Once this is done, we can take the ratio of the confirmed cases and the population and calculate the “confirmed cases per 1 million” statistic and making a heatmap visualisation is straightforward after that.\nJoining data in DS is called “blending data”, which you can find out more here. This is identical to the operation of the various JOIN operations in SQL if you choose your settings correctly.\nAside: of course, joining data isn’t the magic silver bullet for everything. If you examine closely on page 2 of the report, you will see that some countries have null values in the confirmed cases column, e.g. South Korea. This is because the name of the country was not consistent between the two counties. E.g. “South Korea” in one data and “Korea, South” in another. This has to be fixed by making a new column with matched names. The way that I did it was to create a country_correct column in the population data using the definitions such as\nCASE WHEN country IN (\u0026quot;United States\u0026quot;) THEN \u0026quot;US\u0026quot; WHEN country IN (\u0026quot;Russian Federation\u0026quot;) THEN \u0026quot;Russia\u0026quot; WHEN country IN (\u0026quot;Iran, Islamic Rep.\u0026quot;) THEN \u0026quot;Iran\u0026quot; ELSE country END You will probably notice that this is SQL code and quite similar to the dplyr function case_when. So I think some knowledge of SQL will be helpful when using DS.\n Making the visualisations This is where I was pleasantly surprised and where a point-and-click dashboard visualisation wins over Shiny. Since the country_region/country_correct is a geography variable, it can be directly plotted onto a world map automatically. Also importantly, linked interactivity is easily achieved between the table and the heatmap on page 2.\nAside: these tasks are possible, butnot always the easiest to do in R, plotly is the most competitive in this area (see this example and this example). Sure, I can pick up the codes in 5 minutes, but think about the learning curve behind learning ggplot/plotly framework, this could be overwhelming for people without a programming background. I can’t tell you how many hours I have spent trying to make a plot look right!\n  Conclusion For the R/Shiny fanatics, put down your pitchforks, I am not going to abandon R/Shiny any time soon. Shiny is definitely great for customisation, but DS and other professional dashboard software actually do have a lot of awesome ideas. Advanced R/Shiny users can do many things that DS can do, but this is only assuming you have enough experience and know where to look for the right answers. For example:\n The fact that I can readily share this report without setting up my own Shiny server or website is a great example of how Google thought about how their products will be distributed!\n I am also not an artist, so I actually dislike spending hours to make my Shiny app look pretty, DS actually makes this process much easier since making a dashboard UI is quite similar to making PowerPoint slides.\n Connecting to databases might be annoying and slow at times (seriously, it takes about 20 seconds for me to fetch the COVID-19 data over the National Broadband Network here in Australia), but when DS tries to fetch data, it has the backings of Google’s massive servers, and making changes to my visualisation is often done in less than 2 seconds!\n  I can see myself using DS to visualise my pre-computed results in R and distribute this kind of interactive reports with others without paying for expensive Shiny servers, assuming that simple visualisations are all that I am after.\n ","date":1586563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586606981,"objectID":"ccc79090f861cc09e7ea509a1a150e06","permalink":"https://www.kevinwangstats.com/post/visualising-covid19-using-data-studio/","publishdate":"2020-04-11T00:00:00Z","relpermalink":"/post/visualising-covid19-using-data-studio/","section":"post","summary":"Introduction tl;dr: Google’s Data Studio is a great (and free) option for making data visualisation dashboards. Being a web-based dashboard application, its ability to connect to other Google products, such as BigQuery and GCP resources offers huge advantage over competing products.","tags":["datavis","GCP"],"title":"Visualising COVID-19 data using Data Studio","type":"post"},{"authors":[],"categories":[],"content":"  Motivation Stranded at home due to COVID-19, I am trying to pick up SQL and Google’s BigQuery as new skills. Even though the learning curve has been okay, I can’t help to think how easy (and fast!) it was for me to use dplyr to do exactly the same operations.\nThe purpose of this post is to document some SQL against dplyr, or more precisely, dbplyr (dplyr verbs on a database, don’t ask, it is magic!) to query COVID-19 data from BigQuery.\nThe main part of the article starts here, if you want to skip over using BigQuery in the command line.\nOverall, I think dplyr offers a more modern syntax. But then, of course, I am very biased towards anything that comes out of tidyverse and keep in mind that dplyr is about 40 years later than SQL and probably borrowed a lot of strength from it.\nWhat you need  A Google Cloud account. See here for some resources and set it up with an appropriate project R packages: dplyr, bigrquery, DBI Command line tools from Google Cloud   Why GCP/BigQuery? Well, Google gave me lots of free credits. I am easily bought.\nJokes aside, I find the GCP documentation to be quite well-written and always up-to-date. Plus, the documentation contains lots examples of how to link the queried data to data studio and machine learning algorithms. Both of these capabilities are things that I want to eventually learn without paying for visualisation softwares like Tableau (sadly, I am no longer a student to qualify for their free accounts).\n  (Optional) Running a BigQuery query in the command line (Note: to use BigQuery, you will need to have a Google Cloud account and enable billing for a project, this might cost you, but luckily Google Cloud offers free credits when you sign up.)\nThe two most common ways of making queries are:\nGoing through the Google Cloud Console and use the WebUI\n Command line\n  I will show you a quick demo of how to run a simple query in the command line. But I do find the web UI option incredibly useful. I can see myself using web UI on the go if I have some urgent data questions while riding on a bus to work (totally normal behaviour based on my past experiences).\nBigQuery has a set of public datasets, including one for COVID-19. The Johns Hopkins University dataset is also part of this collection, and at the time of writing this article, this dataset is small enough for demostration purposes.\nIn command line, tying\nbq show bigquery-public-data:covid19_jhu_csse.summary gives the output\nTable bigquery-public-data:covid19_jhu_csse.summary Last modified Schema Total Rows Total Bytes Expiration Time Partitioning Clustered Fields Labels ----------------- ----------------------------- ------------ ------------- ------------ ------------------- ------------------ -------------- 02 Apr 19:06:19 |- province_state: string 41129 5564578 freebqcovid: |- country_region: string |- date: date |- latitude: float |- longitude: float |- location_geom: geography |- confirmed: integer |- deaths: integer |- recovered: integer |- active: integer |- fips: string |- admin2: string |- combined_key: string  Here, bigquery-public-data is the name of the BigQuery project that hosts the COVID-19 data. The name of the data stored in that project is covid19_jhu_csse.summary. And you can see that the output of bq show gives the specifications of all the columns in this data.\nTo make a normal SQL query, you can use\nbq query --use_legacy_sql=false \\ \u0026#39;SELECT * FROM `bigquery-public-data.covid19_jhu_csse.summary` LIMIT 10;\u0026#39; which gives the output\nWaiting on bqjob_r28ad9889c4311223_0000017139f7f30c_1 ... (0s) Current status: DONE +----------------+----------------------+------------+----------+-----------+---------------+-----------+--------+-----------+--------+------+--------+--------------+ | province_state | country_region | date | latitude | longitude | location_geom | confirmed | deaths | recovered | active | fips | admin2 | combined_key | +----------------+----------------------+------------+----------+-----------+---------------+-----------+--------+-----------+--------+------+--------+--------------+ | NULL | United Arab Emirates | 2020-02-13 | NULL | NULL | NULL | 8 | 0 | 1 | NULL | NULL | NULL | NULL | | NULL | Thailand | 2020-02-22 | NULL | NULL | NULL | 35 | 0 | 17 | NULL | NULL | NULL | NULL | | NULL | Vietnam | 2020-01-24 | NULL | NULL | NULL | 2 | NULL | NULL | NULL | NULL | NULL | NULL | | NULL | Malaysia | 2020-02-17 | NULL | NULL | NULL | 22 | 0 | 7 | NULL | NULL | NULL | NULL | | NULL | Finland | 2020-02-25 | NULL | NULL | NULL | 1 | 0 | 1 | NULL | NULL | NULL | NULL | | NULL | Vietnam | 2020-02-21 | NULL | NULL | NULL | 16 | 0 | 14 | NULL | NULL | NULL | NULL | | NULL | UK | 2020-02-17 | NULL | NULL | NULL | 9 | 0 | 8 | NULL | NULL | NULL | NULL | | NULL | Nepal | 2020-02-21 | NULL | NULL | NULL | 1 | 0 | 1 | NULL | NULL | NULL | NULL | | NULL | San Marino | 2020-02-27 | NULL | NULL | NULL | 1 | 0 | 0 | NULL | NULL | NULL | NULL | | NULL | Thailand | 2020-01-31 | NULL | NULL | NULL | 19 | NULL | 5 | NULL | NULL | NULL | NULL | +----------------+----------------------+------------+----------+-----------+---------------+-----------+--------+-----------+--------+------+--------+--------------+ Not bad! But let’s try to do this in R.\n Connecting to BigQuery using DBI The DBI (DataBaseInterface) package provides an important function, dbconnect, which facilitates the connection to various databases. The most important argument in dbconnect() is drv, which specifies the driver that is necessary to connect to a database. The extra arguments in this case are related to Google’s way of setting up a Google Cloud projects and billing: + project = \"bigquery-public-data\" sets which project the data is at, in this case, it is at the project managed by Google called “bigquery-public-data”. + dataset = \"covid19_jhu_csse\" refers to the dataset (consiste of multiple tables) stored in the project. + billing = \"your_project_name\" refers to billing account that you have with Google. Since you are making queries to databases, which takes up computational resources, which are not free, so you will need an Google Cloud account and your own project so Google will charge you. Mine is called “scpworkshop”.\nlibrary(DBI) library(bigrquery) con \u0026lt;- dbConnect( drv = bigrquery::bigquery(), project = \u0026quot;bigquery-public-data\u0026quot;, dataset = \u0026quot;covid19_jhu_csse\u0026quot;, billing = \u0026quot;scpworkshop\u0026quot; ) We can list all the tables in the dataset using the dbListTables function.\ndbListTables(con) ## List all tables in this connection ## Using an auto-discovered, cached token. ## To suppress this message, modify your code or options to clearly consent to the use of a cached token. ## See gargle\u0026#39;s \u0026quot;Non-interactive auth\u0026quot; vignette for more details: ## https://gargle.r-lib.org/articles/non-interactive-auth.html ## The bigrquery package is using a cached token for kevin.wang09@gmail.com. ## [1] \u0026quot;confirmed_cases\u0026quot; \u0026quot;deaths\u0026quot; \u0026quot;recovered_cases\u0026quot; \u0026quot;summary\u0026quot; Once we decide which table we wish to work on, in this case, the summary table, we can check on the column types of this table first before making the query. The reason that I prefer to check the column types first is because I keep finding a parsing error with the latest version of the bigrquery package, which I have fixed here. If you want to follow the example below, please install my updated package using devtools::install_github(\"kevinwang09/bigrquery\", ref = \"geography\").\nbigrquery::bq_table_fields(\u0026quot;bigquery-public-data.covid19_jhu_csse.summary\u0026quot;) ## List field types  ## \u0026lt;bq_fields\u0026gt; ## province_state \u0026lt;STRING\u0026gt; ## country_region \u0026lt;STRING\u0026gt; ## date \u0026lt;DATE\u0026gt; ## latitude \u0026lt;FLOAT\u0026gt; ## longitude \u0026lt;FLOAT\u0026gt; ## location_geom \u0026lt;GEOGRAPHY\u0026gt; ## confirmed \u0026lt;INTEGER\u0026gt; ## deaths \u0026lt;INTEGER\u0026gt; ## recovered \u0026lt;INTEGER\u0026gt; ## active \u0026lt;INTEGER\u0026gt; ## fips \u0026lt;STRING\u0026gt; ## admin2 \u0026lt;STRING\u0026gt; ## combined_key \u0026lt;STRING\u0026gt; DBI::dbGetQuery(con, \u0026quot;SELECT * FROM `bigquery-public-data.covid19_jhu_csse.summary` LIMIT 10;\u0026quot;) ## # A tibble: 10 x 13 ## province_state country_region date latitude longitude location_geom ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Hubei Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 2 Guangdong Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 3 Zhejiang Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 4 Henan Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 5 Chongqing Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 6 Hunan Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 7 Beijing Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 8 Anhui Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 9 Shandong Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## 10 Sichuan Mainland China 2020-01-26 NA NA \u0026lt;NA\u0026gt; ## # … with 7 more variables: confirmed \u0026lt;int\u0026gt;, deaths \u0026lt;int\u0026gt;, recovered \u0026lt;int\u0026gt;, ## # active \u0026lt;int\u0026gt;, fips \u0026lt;chr\u0026gt;, admin2 \u0026lt;chr\u0026gt;, combined_key \u0026lt;chr\u0026gt;  Comparing the syntax style of dplyr and SQL Suppose we want to check the total confirmed (the columns are already in cumulative confirmed cases) COVID19 cases for Italy and Spain, then in SQL, we can make the following query:\nsql_tbl = DBI::dbGetQuery(con, \u0026quot;SELECT country_region, confirmed FROM `bigquery-public-data.covid19_jhu_csse.summary` WHERE country_region IN (\u0026#39;Italy\u0026#39;, \u0026#39;Spain\u0026#39;) AND date = \u0026#39;2020-04-02\u0026#39; \u0026quot;) sql_tbl ## # A tibble: 2 x 2 ## country_region confirmed ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Italy 115242 ## 2 Spain 112065 Looks good!\nBut what about dplyr?\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union covid_data = tbl(con, \u0026quot;summary\u0026quot;) dplyr_tbl = covid_data %\u0026gt;% dplyr::filter(country_region %in% c(\u0026#39;Italy\u0026#39;, \u0026#39;Spain\u0026#39;), date == \u0026#39;2020-04-02\u0026#39;) %\u0026gt;% dplyr::select(country_region, confirmed) %\u0026gt;% collect() ## collect() pulls the entire table into memory dplyr_tbl ## # A tibble: 2 x 2 ## country_region confirmed ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Italy 115242 ## 2 Spain 112065 But are the results the same?\nall.equal(sql_tbl, dplyr_tbl) ## [1] TRUE Awesome!\nWe can see that both dplyr verbs and SQL in this query are very similar which is great for data scientist to translate between the two.\nA more complicated query You might be asking why I didn’t put US and China in the query, well, this is because that both countries has state/province level data, so the total confirmed caes will need to be summed across the state/province level first before being compared to other data at the country level.\nCombine this subuery with the previous query, we get:\nsql_tbl = DBI::dbGetQuery(con, \u0026quot;SELECT country_region, SUM(confirmed) AS country_confirmed FROM `bigquery-public-data.covid19_jhu_csse.summary` WHERE country_region IN (\u0026#39;Italy\u0026#39;, \u0026#39;Spain\u0026#39;, \u0026#39;US\u0026#39;, \u0026#39;China\u0026#39;) AND date = \u0026#39;2020-04-02\u0026#39; GROUP BY country_region ORDER BY country_confirmed; \u0026quot;) sql_tbl ## # A tibble: 4 x 2 ## country_region country_confirmed ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 China 82432 ## 2 Spain 112065 ## 3 Italy 115242 ## 4 US 243453 Not bad! The UNION ALL took care a lot of the hard work!\nIn dplyr, the job is (I would argue) simpler:\ndplyr_tbl = covid_data %\u0026gt;% filter(country_region %in% c(\u0026#39;Italy\u0026#39;, \u0026#39;Spain\u0026#39;, \u0026#39;US\u0026#39;, \u0026#39;China\u0026#39;), date == \u0026#39;2020-04-02\u0026#39;) %\u0026gt;% group_by(country_region) %\u0026gt;% summarise(country_confirmed = sum(confirmed)) %\u0026gt;% arrange(country_confirmed) %\u0026gt;% collect() ## Warning: Missing values are always removed in SQL. ## Use `SUM(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. dplyr_tbl ## # A tibble: 4 x 2 ## country_region country_confirmed ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 China 82432 ## 2 Spain 112065 ## 3 Italy 115242 ## 4 US 243453 all.equal(sql_tbl, dplyr_tbl) ## [1] TRUE  Which is better? Very much up to your personal taste!\nI personally prefer dplyr because I think its design is more intuitive:\n the grouping variable is automatically included in the final output, without using extra selection of columns\n the operation of selecting columns and summarising columns are two distinct verbs in dplyr, whereas in SQL it is just SELECT\n   pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }    ","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585738396,"objectID":"e5a021cf9a894fb5b5ec1cb25c346ce4","permalink":"https://www.kevinwangstats.com/post/bigquery-in-r/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/bigquery-in-r/","section":"post","summary":"Motivation Stranded at home due to COVID-19, I am trying to pick up SQL and Google’s BigQuery as new skills. Even though the learning curve has been okay, I can’t help to think how easy (and fast!","tags":["GCP","Cloud"],"title":"BigQuery in R","type":"post"},{"authors":[],"categories":[],"content":"  Introduction The recent COVID-19 outbreak has caused much disruptions to people’s daily lives. As the policy of self-isolation gets adopted by many countries around the world, many people took to social media to share important resources and data visualisation illustrating the severity of COVID-19. I want to quite clear about my intentions behind this blog post: I am not an epidemiologist/biologist/medical doctor. I will refrain from making any inferences from this data (ironic for a statistician) because it would be irresponsible for me to make commentaries on an ongoing public health crisis in which I am not an expert on. I am only here to show you some interesting R coding and data visualisations.\nBetween mid-February 2020 and mid-March 2020, I was in Cornell University (New York state) and observing the spread of COVID-19 quite closely. I was increasingly worried about the dramatic increase in the number of cases in US and the potential shutdown of the Australian border. I was on the brink of re-booking all my flights before it is too late. It is around the same time that I was asked by my supervisor back in Australia to design a lecture in Shiny apps, so I thought it will be useful for me to write an app and other visualisations to answer the following questions:\nWhat do the confirmed cases for each country looks like? What is the days-lagged in confirmed cases for each compare when compare to China (i.e. cross-correlation)? What are the Sydney-bound flights that had confirmed cases? Is there a route that is safer than others?   Shiny app for confirmed cases and added cases This app attemps to answer the first question, code: https://github.com/kevinwang09/covid19. I can’t afford a server at this point, so you will need to run this app locally by reading through the instructions in the README of that repo.\nBased on my simple visualisations at the time (~15 March 2020), I estimated that US’s major outbreak lags behind that of China by about 45 days or so. So it wasn’t so dangerous for me when I was in Cornell around mid-March, however, it was definitely not ideal as the county I was in already had two confirmed cases. Any delays in my departure could spell trouble. This is unfortunately true since at the time of writing, US has overtaken China in confirmed cases and New York state shares the biggest percentage of those confirmed cases.\nThe structure of the app is quite simple:\n The COVID-19 data is fetched using the nCov2019 package using this line of code here Cumulative confirmed cases are extracted here and time series plot is made here and the cross-correlation plot is made here. Similarly, the plots for added cases are here and here.   Interactive animation of flights with confirmed case This is a standalone RMarkdown document: https://kevinwang09.github.io/covid19/confirmed_flights.html#5_plotly_visualisations.\nThe New South Wales Health website publishes a list of flights with confirmed cases of COVID-19.\nThe coding beind this visualisation is also quite straight-forward:\n The data are scrapped from the NSW Health website using the xml2 and rvest packages. I particularly like the elegance of the coding style using tidyver to scrap this data, though some inspiration came from this StackOverflow thread  url = \u0026quot;https://www.health.nsw.gov.au/Infectious/diseases/Pages/coronavirus-flights.aspx\u0026quot; raw = xml2::read_html(url) raw_flights_tbl = raw %\u0026gt;% rvest::html_node(xpath = \u0026quot;.//div[@id=\u0026#39;ctl00_PlaceHolderMain_contentc1__ControlWrapper_RichHtmlField\u0026#39;]/table\u0026quot;) %\u0026gt;% rvest::html_table() %\u0026gt;% as_tibble() %\u0026gt;% janitor::clean_names()  The geographical locations are then queried through Google Maps API for their longitude and latitudes.  all_geocode = tibble( location = c(flights_tbl$origin, flights_tbl$destination) %\u0026gt;% unique, geocode = purrr::map(location, ggmap::geocode))  After grabbing these data, it is time to make plots using this chunk of code. Althought a lot of small adjustments were needed, but this example from Plotly’s official website was quite helpful.   ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585304510,"objectID":"5e94475d3fc57114e647a76920598742","permalink":"https://www.kevinwangstats.com/post/covid-19-visualisations/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/post/covid-19-visualisations/","section":"post","summary":"Introduction The recent COVID-19 outbreak has caused much disruptions to people’s daily lives. As the policy of self-isolation gets adopted by many countries around the world, many people took to social media to share important resources and data visualisation illustrating the severity of COVID-19.","tags":["datavis","shiny"],"title":"COVID-19 visualisations","type":"post"},{"authors":null,"categories":null,"content":"The goal of learningtower is to provide a user-friendly R package to provide easy access to a subset of variables from PISA data collected from the OECD, for the years 2000 - 2018, collected on a three year basis. This is an excellent dataset for exploring and visualizing data. This dataset can also be used for other analytically purposes as well as statistical computations.\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"9e607cc5e9e0130760c96add7f470208","permalink":"https://www.kevinwangstats.com/package/learningtower/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/package/learningtower/","section":"package","summary":"Curated OECD-PISA student assessment data (2000-2018)","tags":null,"title":"learningtower","type":"package"},{"authors":null,"categories":null,"content":"Multi-collinearity is a big problem in performing regression. Without properly detecting collinearity, the regression estimation could be severely biased. The traditional statistics like variance inflation factor or eigenvalues do not always perform well in applications. mcvis is a new framework that utilises resampling techniques to repeatedly learn from these conventional collinearity indices and their relationships in order to better understand the causes of collinearity. Our framework is made available in R through the mcvis package which includes new collinearity measures and visualisations, in particular a bipartite plot that informs on the degree and structure of collinearity.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2f50d99e778892c01f3df6d59e0e0327","permalink":"https://www.kevinwangstats.com/package/mcvis/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/package/mcvis/","section":"package","summary":"multicollinearity visualisation","tags":null,"title":"mcvis","type":"package"},{"authors":null,"categories":null,"content":"Single-cell RNA-seq is an exciting new technology that allows us to study cell heterogeneity. However, like most transcriptomics data, multiple scRNA-Seq data cannot be easily combined without adjustments for scaling. scMerge uses clever algorithms to group similar cells between multiple datasets to understand the noise structure between them. This noise structure is then removed from these datasets to produce a merged data suitable for other downstream scRNA-Seq data analysis.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ac83946b393e373eaf088027b4a4351d","permalink":"https://www.kevinwangstats.com/package/scmerge/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/package/scmerge/","section":"package","summary":"Merging single-cell RNA-Seq data","tags":null,"title":"scMerge","type":"package"},{"authors":null,"categories":null,"content":"I have an interest in developing Bioconductor packages. I am currently the maintainer of scMerge. I am thinking about working towards integrating tidyverse concepts with S4-class objects in Bioconductor, e.g. through plyexperiment.\nI think the Bioconductor framework for package management (and by extension, the pursue for reproducibility in research) is invaluable. Thus, I have taken up some extra efforts to learn about the git version control system and the package continuous integration tool travis. I also use docker, which you can read about here.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7773e204563ae2eccd2adffd19c3fcd1","permalink":"https://www.kevinwangstats.com/project/bioc/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/bioc/","section":"project","summary":"Bioconductor-styled packages for research","tags":null,"title":"Bioconductor packages","type":"project"},{"authors":null,"categories":null,"content":"I am in the process of learning the pros/cons between R and Python/SQL for a range of analytical tasks. As such, I have created this webpage for R vs Python and this webpage for R vs SQL.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ece43116c7885e27e39017a6fc3f2538","permalink":"https://www.kevinwangstats.com/project/rpython/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/rpython/","section":"project","summary":"Compare coding styles of R, Python and SQL","tags":null,"title":"Compare R \u0026 Python/SQL","type":"project"},{"authors":null,"categories":null,"content":"I always enjoy watching cricket and given that my research is in bioinformatics, I have \u0026ldquo;created\u0026rdquo; my own discipline: Cricinformatics (the name has nothing to do with Cricinfo, but I am open to the idea of a sponsorship).\nCricinformatics aims to use powerful bioinformatics tools to analyse cricket data. I have written a talk on this and all codes are available on GitHub.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fb7bdd964f8a803a6b427468a92c5aa3","permalink":"https://www.kevinwangstats.com/project/cricinformatics/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cricinformatics/","section":"project","summary":"Applying bioinformatics tools to cricket","tags":null,"title":"Cricinformatics","type":"project"},{"authors":null,"categories":null,"content":"I like make pretty pictures to explain statistical concepts. This project is very broad but here is a selection of my works to visualise:\n Multi-collinearity Duality between p-values and confidence interval Learning ggplot2 using the learnr package Visualising Elastic Net (in development)  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"89930693e410374e2cf47f5a90e56c78","permalink":"https://www.kevinwangstats.com/project/visualisation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/visualisation/","section":"project","summary":"Visualisation of complex data/methods","tags":null,"title":"Data Visualisation","type":"project"},{"authors":null,"categories":null,"content":"One of the areas of research that I have a particular focus on is feature selection. One of my more notable work on this is APES - APproximated Exhaustive Search for generalised linear models. It is available as a standalone R package.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9f899240b6b5326e20c0d290ad3f127d","permalink":"https://www.kevinwangstats.com/project/featureselection/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/featureselection/","section":"project","summary":"Various techniques in my research","tags":null,"title":"Feature Selection","type":"project"},{"authors":null,"categories":null,"content":"During my PhD years, I realised that I don\u0026rsquo;t get enough exercise. And given that my research is in bioinformatics, I have a great idea to play Pokemon Go and analyse the data/theory associated with the game.\nI have given talks on the classical Coupon Collector\u0026rsquo;s Problem in statistics, which can be seen as a basic model for collecting Pokemons.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"4e6192df0f26862f0213ffa6ecba7efb","permalink":"https://www.kevinwangstats.com/project/pokemon/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/pokemon/","section":"project","summary":"Applying bioinformatics to Pokemon data","tags":null,"title":"Pokemon Data","type":"project"},{"authors":null,"categories":null,"content":"I am using Python to solve a series of Project Euler challenges. All codes and explanations are stored in this GitHub repository.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"4bdbb4cce664c2f29b0b372e535dc80a","permalink":"https://www.kevinwangstats.com/project/project_euler/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/project_euler/","section":"project","summary":"A collection of maths/coding challenges","tags":null,"title":"Project Euler","type":"project"},{"authors":null,"categories":null,"content":"I believe research should always be reproducible, even if that translates to extra work for the authors/developers. That line of thinking lead me to learn about docker. I have used it in the past to produce containers to freeze old versions of my package (e.g. see docker image for scMerge).\nSome Google Cloud based workshops I have designed and administered are on the topics of\n  Single Cell RNA Analysis. Which was hosted at the University of Sydney, Bioinformatics Summer 2019 and Cornell University. An accompany wiki page is here.\n  R, RStudio and Tidyverse. Which was hosted at Bioinformatics Summer 2019.\n  I also helped with the deployment of the following workshops at Bioinformatics Summer 2019:\n HiC analysis Phosphoproteomic data analysis  Every workshop that I hosted come with a GitHub Page website. All scripts for running the workshop (except passwords) are stored in their respective GitHub repo.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"258db080f248896e25eb16e37c53720c","permalink":"https://www.kevinwangstats.com/project/cloud_computing/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/cloud_computing/","section":"project","summary":"Using Docker and the Cloud","tags":null,"title":"Projects in the cloud","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"791b23a3e028dde8054fee33f3e45504","permalink":"https://www.kevinwangstats.com/project/tidytuesday/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/tidytuesday/","section":"project","summary":"Weekly data project from R For Data Science","tags":null,"title":"tidytuesday","type":"project"},{"authors":null,"categories":null,"content":"Exhaustive variable selection is known to be time consuming, especially for Generalised Linear Models (GLMs). APES is a variable selection method that first converts a given GLM into a linear model first and then uses a best-subset algorithm to find the best linear model. This linear model is then converted back to a GLM to approximate the original exhaustive search problem. APES can be orders of magnitudes faster than the true exhaustive search while retaining a reasonable accuracy.\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"e8fa3fdc8f64b251d88132bc41952afb","permalink":"https://www.kevinwangstats.com/package/apes/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/package/apes/","section":"package","summary":"Approximated Exhaustive Search for GLMs","tags":null,"title":"APES","type":"package"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://www.kevinwangstats.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]